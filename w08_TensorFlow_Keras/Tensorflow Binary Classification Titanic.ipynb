{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf0aaf0-f556-4bf2-9a56-5e84e4096d55",
   "metadata": {},
   "source": [
    "# Binary Classification using Tensorflow\n",
    "\n",
    "TensorFlow is a versatile framework for machine learning, capable of tackling diverse tasks, including binary classification. Using the Titanic dataset, the objective is to predict whether a passenger survived or not, based on features like:\n",
    "- **Age**, **Gender**, **Passenger Class (Pclass)**, **Fare**, **Embarked Port**, and more.\n",
    "\n",
    "### Key Steps in the Process\n",
    "\n",
    "1. **Data Preprocessing**  \n",
    "   - Cleaning and handling missing values.  \n",
    "   - Encoding categorical variables (e.g., gender or embarkation ports).  \n",
    "2. **Model Building**  \n",
    "   - Designing a neural network specifically for binary classification.  \n",
    "   - Using layers like Dense, Activation, and Dropout to create a flexible model.  \n",
    "3. **Training and Evaluation**  \n",
    "   - Leveraging TensorFlow's tools to train the model using labelled data.  \n",
    "   - Evaluating its performance on unseen data to ensure generalisation.\n",
    "------\n",
    "We are going to do similar to the Regression Tensorflow workbook, but this time we are going to work at a classification problem\n",
    "\n",
    "Back to the titanic.csv file we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8530745d-eefb-4de6-9b27-2c5ce3dc331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import tensorflow as tf # TensorFlow - Main library for building and training neural networks\n",
    "import pandas as pd # pandas - for data manipulation and handling structured datasets\n",
    "import numpy as np # NumPy - support for numerical operations and working with arrays\n",
    "import matplotlib.pyplot as plt # Matplotlib - for visualizing data and model performance\n",
    "from sklearn.model_selection import train_test_split # utility from scikit-learn to split data into training and test sets\n",
    "\n",
    "# Print TensorFlow version to confirm installation and compatibility\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655b0f5-3c4d-4e03-8e14-4576b675ed28",
   "metadata": {},
   "source": [
    "### Read and Clean Data\n",
    "\n",
    "To prepare the dataset for modelling, we'll follow these steps:\n",
    "\n",
    "1. **Read in the data**: Load the Titanic dataset using a suitable library (e.g., `pandas`).\n",
    "2. **Handle missing values**:\n",
    "   - Remove rows where the `Embarked` column is not filled.\n",
    "   - Fill in missing values in the `Age` column with the median age.\n",
    "3. **Select relevant features**:\n",
    "   - Identify and keep only the columns required for building the model.\n",
    "4. **Encode categorical variables**:\n",
    "   - Use a `LabelEncoder` to convert the `Sex` column from text (e.g., \"male\", \"female\") to numbers.\n",
    "   - Use a `OneHotEncoder` to encode the `Embarked` column into multiple binary columns (one per embarkation point).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19ea013f-077d-4450-be3c-b293dc82051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and clean Data ----------\n",
    "# Load Titanic dataset\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "# Remove rows with missing 'Embarked' values and fill missing 'Age' with the median\n",
    "df = df[df[\"Embarked\"].notnull()]\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "# Define target variable 'y' (Survived) and features 'X'\n",
    "y = df[\"Survived\"]\n",
    "X = df[[\"Pclass\", \"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e7a68",
   "metadata": {},
   "source": [
    "Convert sex to numeric values using label encoder from preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "799a4138-872f-467e-b527-697ec72162b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Encode the 'Sex' column using LabelEncoder\n",
    "le_sex = preprocessing.LabelEncoder()  # Initialize the encoder\n",
    "le_sex.fit(X['Sex'])                   # Fit the encoder on the 'Sex' column\n",
    "sex = le_sex.transform(X['Sex'])       # Transform 'Sex' into numerical values (e.g., 0 for male, 1 for female)\n",
    "\n",
    "# Replace the original 'Sex' column with the encoded values\n",
    "X = X.drop(['Sex'], axis=1)  # Drop the original 'Sex' column\n",
    "X['Sex'] = sex               # Add the encoded 'Sex' column back\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44252c-98fc-4357-b66e-5aafb4e05316",
   "metadata": {},
   "source": [
    "Convert Embarked to numeric values using OneHotEncoder from preprocessing library. There's probably a better way of doing this but this'll work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f453c351-8119-459e-b2c1-8f7d441f1ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex</th>\n",
       "      <th>EmbarkC</th>\n",
       "      <th>EmbarkQ</th>\n",
       "      <th>EmbarkS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass   Age  SibSp  Parch     Fare  Sex  EmbarkC  EmbarkQ  EmbarkS\n",
       "0         3  22.0      1      0   7.2500    1      0.0      0.0      1.0\n",
       "1         1  38.0      1      0  71.2833    0      1.0      0.0      0.0\n",
       "2         3  26.0      0      0   7.9250    0      0.0      0.0      1.0\n",
       "3         1  35.0      1      0  53.1000    0      0.0      0.0      1.0\n",
       "4         3  35.0      0      0   8.0500    1      0.0      0.0      1.0\n",
       "..      ...   ...    ...    ...      ...  ...      ...      ...      ...\n",
       "886       2  27.0      0      0  13.0000    1      0.0      0.0      1.0\n",
       "887       1  19.0      0      0  30.0000    0      0.0      0.0      1.0\n",
       "888       3  28.0      1      2  23.4500    0      0.0      0.0      1.0\n",
       "889       1  26.0      0      0  30.0000    1      1.0      0.0      0.0\n",
       "890       3  32.0      0      0   7.7500    1      0.0      1.0      0.0\n",
       "\n",
       "[889 rows x 9 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Encode the 'Embarked' column using OneHotEncoder\n",
    "le_embark = preprocessing.OneHotEncoder(sparse_output=False)  # Initialize encoder with dense output\n",
    "le_embark.fit(X[\"Embarked\"].values.reshape(-1, 1))            # Fit encoder on the 'Embarked' column\n",
    "embarked = le_embark.transform(X[\"Embarked\"].values.reshape(-1, 1))  # Transform 'Embarked' into one-hot vectors\n",
    "\n",
    "# Replace the original 'Embarked' column with one-hot encoded columns\n",
    "X = X.drop([\"Embarked\"], axis=1)      # Drop the original 'Embarked' column\n",
    "X[\"EmbarkC\"] = embarked[:, 0]         # Add column for 'C' (Cherbourg)\n",
    "X[\"EmbarkQ\"] = embarked[:, 1]         # Add column for 'Q' (Queenstown)\n",
    "X[\"EmbarkS\"] = embarked[:, 2]         # Add column for 'S' (Southampton)\n",
    "X  # Display the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee179b-a06e-4e03-ba2d-1b407471f4a3",
   "metadata": {},
   "source": [
    "### Machine Learning Process\n",
    "\n",
    "Now we can proceed with the Machine Learning workflow. The steps include:\n",
    "\n",
    "1. **Train-Test Split**: Divide the dataset into training and testing sets to evaluate model performance on unseen data.\n",
    "2. **Normalise the Data**: Scale the features to ensure consistent ranges, improving model convergence during training.\n",
    "3. **Build Base Models**: Define initial neural network architectures for binary classification.\n",
    "4. **Compile**: Configure the model with an optimiser, loss function, and evaluation metrics.\n",
    "5. **Fit**: Train the model on the training data, adjusting weights based on the loss function.\n",
    "6. **Evaluate**: Test the model on the test set to assess its performance.\n",
    "\n",
    "These steps follow the typical train-test split approach to ensure fair evaluation and generalisation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6500f346-5a5f-47c6-932b-588b0753679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1138, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1ad22-5206-4101-982f-00809c56c08f",
   "metadata": {},
   "source": [
    "Normalisation like was done on the Regression workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "907cbf1f-3018-4129-8593-10718dfc5f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: [[-1.5950363   0.1471719  -0.46338156 ... -0.4819084  -0.3198837\n",
      "   0.6256112 ]\n",
      " [ 0.80765593 -1.572492   -0.46338156 ... -0.4819084  -0.3198837\n",
      "   0.6256112 ]\n",
      " [ 0.80765593 -0.08732773 -0.46338156 ...  2.0750833  -0.3198837\n",
      "  -1.5984368 ]\n",
      " ...\n",
      " [-1.5950363  -0.08732773 -0.46338156 ... -0.4819084  -0.3198837\n",
      "   0.6256112 ]\n",
      " [ 0.80765593 -0.7126601  -0.46338156 ... -0.4819084  -0.3198837\n",
      "   0.6256112 ]\n",
      " [-0.39369014 -2.1978242   1.3031831  ... -0.4819084  -0.3198837\n",
      "   0.6256112 ]]\n"
     ]
    }
   ],
   "source": [
    "###Error### >from tensorflow.keras.layers.experimental import preprocessing<\n",
    "# The tensorflow.keras.layers.experimental.preprocessing module has been deprecated in recent TensorFlow versions. \n",
    "\n",
    "# The Normalization layer is now part of tensorflow.keras.layers. Here's the corrected code:\n",
    "from tensorflow.keras.layers import Normalization\n",
    "# Initialize the Normalization layer\n",
    "normalizer = Normalization()\n",
    "# Adapt the normalizer to the training data (compute mean and variance)\n",
    "normalizer.adapt(X_train.values)\n",
    "# Apply normalization to the training data and print the result\n",
    "print('Normalized:', normalizer(X_train.values).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27792c3-0c0d-4585-b344-47343c857398",
   "metadata": {},
   "source": [
    "## Model Number 1 - Logistic Regression\n",
    "We will begin with a simple Logistic Regression model as a baseline.\n",
    "### Key Characteristics:\n",
    "- Logistic Regression is a linear model used for binary classification.\n",
    "- The output layer consists of **one unit** (neuron) with a sigmoid activation function to produce probabilities.\n",
    "Mathematically, the model represents:\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = w^Tx + b$$\n",
    "This formulation calculates the log-odds of the probability \\(p\\), where \\(w\\) represents the weights, \\(x\\) is the input data, and \\(b\\) is the bias term.\n",
    "### TensorFlow Implementation:\n",
    "- The model architecture will include a normalisation layer to scale the input data before applying the logistic regression layer.\n",
    "- This approach is conceptually similar to using `sklearn`'s `LogisticRegression()`.\n",
    "\n",
    "Including the normaliser as a layer ensures data is processed consistently during both training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c12cae7-fc05-45ad-b85e-a8d2b2c76503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">711</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)               │            <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_3 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;34m711\u001b[0m, \u001b[38;5;34m9\u001b[0m)               │            \u001b[38;5;34m19\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19</span> (80.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19\u001b[0m (80.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19</span> (80.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m19\u001b[0m (80.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a Sequential model for logistic regression\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,  # Normalization layer to preprocess input data\n",
    "    # Logistic regression layer\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd6312-cbb5-4a77-89bf-c065a4581495",
   "metadata": {},
   "source": [
    "### Understanding Model Output and Loss Function\n",
    "\n",
    "1. **Trainable Parameters**:  \n",
    "   - The model has **10 trainable parameters** because we have:\n",
    "     - **9 features** (one weight for each feature).\n",
    "     - **1 bias** term (\\(b\\)). (probability of survival if all weights are 0)\n",
    "\n",
    "2. **Loss Function**:  \n",
    "   - We use `BinaryCrossentropy` from TensorFlow because this is a binary classification problem.  \n",
    "   - Documentation: [BinaryCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy).\n",
    "\n",
    "3. **Using `from_logits`**:  \n",
    "   - The `from_logits` parameter determines whether the output of the model is **logits** (unscaled values) or **probabilities** (scaled between 0 and 1).  \n",
    "   - If no activation function is applied to the output layer (e.g., no `sigmoid`), the output is **logits**.  \n",
    "     - In this case, set `from_logits=True`. This is **recommended** by TensorFlow for better numerical stability.  \n",
    "   - If you apply a `sigmoid` activation in the output layer (e.g., `Dense(1, activation='sigmoid')`), the output will be probabilities.  \n",
    "     - Here, set `from_logits=False`.\n",
    "\n",
    "4. **Conclusion**:  \n",
    "   - In the current implementation (`Dense(1)` without an activation), the model outputs logits, so you should use `from_logits=True` in the loss function.\n",
    "   - If you decide to use `Dense(1, activation='sigmoid')`, update the loss function with `from_logits=False` (though TensorFlow recommends avoiding this setup for stability reasons).\n",
    "\n",
    "**Key Tip**: Always align the `from_logits` parameter in the loss function with the activation used (or not used) in the model's output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b285c82f-461c-495b-8c51-87e4a50186e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6043 - loss: 0.7130  \n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7964 - loss: 0.4653 \n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7856 - loss: 0.4652 \n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8177 - loss: 0.4606 \n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8106 - loss: 0.4263 \n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8064 - loss: 0.4254 \n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8198 - loss: 0.4158 \n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8026 - loss: 0.4660 \n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8127 - loss: 0.4585 \n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8195 - loss: 0.4194 \n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7814 - loss: 0.4754 \n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8151 - loss: 0.4159 \n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.4644 \n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.4348 \n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7966 - loss: 0.4375 \n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8040 - loss: 0.4311 \n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.4455 \n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8198 - loss: 0.4395 \n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7866 - loss: 0.4756 \n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8157 - loss: 0.4278 \n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8102 - loss: 0.4719 \n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8259 - loss: 0.4175 \n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8077 - loss: 0.4430 \n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7949 - loss: 0.4530 \n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8330 - loss: 0.4257 \n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8266 - loss: 0.4123 \n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8147 - loss: 0.4584 \n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8174 - loss: 0.4547 \n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8116 - loss: 0.4386 \n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8150 - loss: 0.4212 \n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8127 - loss: 0.4280 \n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8152 - loss: 0.4323 \n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7857 - loss: 0.4535 \n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.4113 \n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8103 - loss: 0.4381 \n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7968 - loss: 0.4477 \n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8000 - loss: 0.4563 \n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7937 - loss: 0.4849 \n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7960 - loss: 0.4686 \n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7954 - loss: 0.4417 \n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8039 - loss: 0.4703 \n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8068 - loss: 0.4409 \n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8157 - loss: 0.4465 \n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8144 - loss: 0.4058 \n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8116 - loss: 0.4594 \n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.4342 \n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7756 - loss: 0.4548 \n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8175 - loss: 0.4317 \n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8191 - loss: 0.4331 \n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7962 - loss: 0.4422 \n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8082 - loss: 0.4487 \n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8266 - loss: 0.4238 \n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7827 - loss: 0.4492 \n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8045 - loss: 0.4659 \n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7917 - loss: 0.4654 \n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8213 - loss: 0.4556 \n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8241 - loss: 0.4061 \n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7967 - loss: 0.4659 \n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7893 - loss: 0.4668 \n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8083 - loss: 0.4303 \n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8169 - loss: 0.4444 \n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8096 - loss: 0.4392 \n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8218 - loss: 0.4253 \n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8143 - loss: 0.4410 \n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8129 - loss: 0.4277 \n",
      "Epoch 66/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8235 - loss: 0.4255 \n",
      "Epoch 67/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8092 - loss: 0.4268 \n",
      "Epoch 68/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8119 - loss: 0.4339 \n",
      "Epoch 69/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8091 - loss: 0.4522 \n",
      "Epoch 70/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4221 \n",
      "Epoch 71/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8057 - loss: 0.4285 \n",
      "Epoch 72/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8186 - loss: 0.4460 \n",
      "Epoch 73/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8138 - loss: 0.4216 \n",
      "Epoch 74/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7989 - loss: 0.4673 \n",
      "Epoch 75/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8196 - loss: 0.4466 \n",
      "Epoch 76/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8006 - loss: 0.4440 \n",
      "Epoch 77/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8101 - loss: 0.4279 \n",
      "Epoch 78/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8248 - loss: 0.4268 \n",
      "Epoch 79/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8136 - loss: 0.4260 \n",
      "Epoch 80/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8323 - loss: 0.4277 \n",
      "Epoch 81/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8228 - loss: 0.4186 \n",
      "Epoch 82/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8079 - loss: 0.4580 \n",
      "Epoch 83/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8055 - loss: 0.4287 \n",
      "Epoch 84/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8164 - loss: 0.4459 \n",
      "Epoch 85/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7950 - loss: 0.4717 \n",
      "Epoch 86/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7992 - loss: 0.4588 \n",
      "Epoch 87/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8038 - loss: 0.4544 \n",
      "Epoch 88/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8216 - loss: 0.4197 \n",
      "Epoch 89/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8353 - loss: 0.4073 \n",
      "Epoch 90/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8139 - loss: 0.4334 \n",
      "Epoch 91/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7789 - loss: 0.4750 \n",
      "Epoch 92/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8117 - loss: 0.4433 \n",
      "Epoch 93/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8281 - loss: 0.4139 \n",
      "Epoch 94/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8069 - loss: 0.4589 \n",
      "Epoch 95/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7946 - loss: 0.4816 \n",
      "Epoch 96/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8039 - loss: 0.4810 \n",
      "Epoch 97/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4318 \n",
      "Epoch 98/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.4673 \n",
      "Epoch 99/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8163 - loss: 0.4339 \n",
      "Epoch 100/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8348 - loss: 0.4345 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fbdbdf7a30>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with optimizer, loss function, and evaluation metrics\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),  # Adam optimizer with learning rate 0.1\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),  # Binary cross-entropy loss with logits\n",
    "    metrics=['accuracy']  # Track accuracy during training -- it must be in []\n",
    ")\n",
    "\n",
    "# Train the model on the training data for 100 epochs\n",
    "model.fit(X_train, y_train, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be15f40-4b2d-4c9b-9115-28dca5d9b784",
   "metadata": {},
   "source": [
    "### Observations on Training Output\n",
    "\n",
    "1. **Loss Decreasing**:  \n",
    "   - The loss decreases consistently across epochs, indicating the model is learning from the training data. However, since no validation set is used, we cannot assess if the model is overfitting.\n",
    "\n",
    "2. **Number of Epochs**:  \n",
    "   - The model was trained for 100 epochs, which might be excessive or insufficient depending on the dataset. Using a validation set and monitoring metrics like validation loss can help determine an optimal number of epochs.\n",
    "\n",
    "3. **Caution on `.fit`**:  \n",
    "   - If you call `.fit` on the same model again, training will **continue from where it left off** (e.g., epoch 101 onwards), as the optimizer retains its state.  \n",
    "   - To start training from scratch, you need to reinitialize the model or reset its weights.\n",
    "\n",
    "4. **Improvements for Future Runs**:  \n",
    "   - Use a **validation set** (e.g., via `validation_split` or a separate test set) to monitor performance and detect overfitting.\n",
    "   - Implement **early stopping** to halt training once the validation loss stops improving, saving computation and avoiding overfitting.\n",
    "\n",
    "5. **Accuracy Plateau**:  \n",
    "   - Accuracy fluctuates slightly around a certain range (e.g., 0.81-0.83), which might indicate the model has reached its capacity with the given architecture and features.\n",
    "   - Consider exploring hyperparameter tuning, additional features, or more complex models if further improvement is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a90d59d-3fe6-43b4-b03d-44d2463ea150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7708 - loss: 0.5493 \n",
      "\n",
      "Logistic Regression Loss:  0.516 Accuracy:  0.7865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5160284638404846, 0.7865168452262878]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the logistic regression model on the test set\n",
    "logRegEval = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the test loss and accuracy, rounded for readability\n",
    "print(\"\\nLogistic Regression Loss: \", round(logRegEval[0], 4), # logRegEval[0]: The loss from model.evaluate() rounded to 4 decimals.\n",
    "      \"Accuracy: \", round(logRegEval[1], 4)) # logRegEval[1]: The accuracy from model.evaluate() rounded to 4 decimals.\n",
    "\n",
    "logRegEval # raw output loss first, accuracy second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e633e79",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "\n",
    "**Accuracy**: 78.65%  \n",
    "- Slightly lower than training (~83%), indicating mild overfitting.\n",
    "\n",
    "**Loss**: 0.516  \n",
    "- Moderate, showing predictions are reasonably close to true labels.\n",
    "\n",
    "### Suggestions\n",
    "- Use a validation set to monitor overfitting.\n",
    "- Apply regularisation or tune hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ae2d8-721e-4f4b-942a-5c7e3f2015bd",
   "metadata": {},
   "source": [
    "# ---------------------------- Folyt!!! -----------------------------------\n",
    "## Model Number 2 - Neural Network\n",
    "\n",
    "Now we are going to build a Neural Network\n",
    "\n",
    "4 layers, 100 units in each hidden layer with relu as the activation function (you can try different structures if you want but for now I'm just demonstrating how Tensorflow works as well as some other things we can try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bee31b6d-4495-45ee-836a-0fa7f3b34d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22117e31-a301-479d-96ed-33674ebea44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization_3 (\u001b[38;5;33mNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │            \u001b[38;5;34m19\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19</span> (80.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19\u001b[0m (80.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19</span> (80.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m19\u001b[0m (80.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc128a47-61c8-4adb-825a-725999cc03ce",
   "metadata": {},
   "source": [
    "21301 trainable params, a lot more than the previous 10! Let's look at each layer\n",
    "\n",
    "Input has 9 features Hidden Layer 1 has 100 units. Therefore the weight matrix is going to have 100 rows and 9 columns to match\n",
    "$$ Wx + b $$\n",
    "\n",
    "That gives us 900 paramaters for the weights. Then we have 100 biases - giving the total of 1000 parameters to go from input layer to hidden layer 1\n",
    "\n",
    "Now from hidden layer 1 to hidden layer 2. 100 features, 100 units, gives us a 100x100 matrix so 10000 parameters, add in the 100 biases and we get the 10100\n",
    "\n",
    "Let's compile and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a09f2216-a1aa-43a6-979f-cebaa4154c33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected `metrics` argument to be a list, tuple, or dict. Received instead: metrics=accuracy of type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBinaryCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model_tf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\norbe\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\norbe\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:133\u001b[0m, in \u001b[0;36mCompileMetrics.__init__\u001b[1;34m(self, metrics, weighted_metrics, name, output_names)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metrics \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metrics, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `metrics` argument to be a list, tuple, or dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived instead: metrics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(metrics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weighted_metrics \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    138\u001b[0m     weighted_metrics, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `weighted_metrics` argument to be a list, tuple, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict. Received instead: weighted_metrics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweighted_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(weighted_metrics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected `metrics` argument to be a list, tuple, or dict. Received instead: metrics=accuracy of type <class 'str'>"
     ]
    }
   ],
   "source": [
    "model_tf.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "model_tf.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8b694-eb05-45ae-a9c5-231691e6291b",
   "metadata": {},
   "source": [
    "Again Loss keeps decreasing, haven't use a validation set though. Could've done a different number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211012af-7694-430b-9f9f-7c08ded1eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Loss: \", round(logRegEval[0],4), \"Accuracy: \", round(logRegEval[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7e463-1636-434b-a476-233ba0f5c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annEval = model_tf.evaluate(X_test, y_test)\n",
    "print(\"ANN Loss: \", round(annEval[0],4), \"Accuracy: \", round(annEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6a46a-cbf0-42ce-98b1-5505f6f16f37",
   "metadata": {},
   "source": [
    "These are potentially overfit, we should have done some validation to check over things\n",
    "\n",
    "While the accuracy for the test data is better, the loss is actually worse!\n",
    "\n",
    "## Using Validation\n",
    "### Logistic Regression with Validation\n",
    "\n",
    "Let's add in validation. kFold Cross Validation would be nice but that's more difficult to do with tensorflow and requires writing our own functions so let's just take the last 20% as a validation set. Since we used train_test_split already, the data is already shuffled so it should be ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aaeb35-5091-445b-99a7-100826d32a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852303c9-a657-4fa4-a1e4-fbb95a3898cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0b326",
   "metadata": {},
   "source": [
    "Same function that was in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca7e16-2c5b-4579-bda3-af5bf4fe6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, which='loss'):\n",
    "    plt.plot(history.history[which], label='train')\n",
    "    try:\n",
    "        plt.plot(history.history['val_'+which], label='validation')\n",
    "    except:\n",
    "        None\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(which)\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523ef11-7739-43ab-bf87-d1ec0bf52a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033eddd8-fc0a-4df3-9428-3e9a237ff951",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegValidationEval = model.evaluate(X_test, y_test)\n",
    "print(\"Logistic Regression with Validation Loss: \", round(logRegValidationEval[0],4), \"Accuracy: \", round(logRegValidationEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abc82a-fee7-4f07-a857-0443af6f8af2",
   "metadata": {},
   "source": [
    "Seems about the same performance on the test set. Your numbers will vary due to SGD\n",
    "\n",
    "Now ANN\n",
    "\n",
    "### ANN with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712facf0-5db9-4e0a-ae9c-8ed7432e8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_tf.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "history = model_tf.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb678ab-f08b-4659-b98c-0f17ce9a832a",
   "metadata": {},
   "source": [
    "Let's plot the loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280fb6a-0ca4-4caf-b68c-b38e0c004e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a7fda-c9ca-4352-a32e-2a0ad789e637",
   "metadata": {},
   "source": [
    "Validation loss is a bit all over the place. This suggests overfitting to me\n",
    "\n",
    "Neural Networks are not always better....even though the training loss and accuracy was better with the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020f88a-83e8-4250-a54d-16e32e1f4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f133db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression with Validation Loss: \", round(logRegValidationEval[0],4), \"Accuracy: \", round(logRegValidationEval[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb4e4a-51bd-42d1-9c41-1ea1b05b273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annValidationEval = model_tf.evaluate(X_test, y_test)\n",
    "print(\"ANN with Validation Loss: \", round(annValidationEval[0],4), \"Accuracy: \", round(annValidationEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9840a0-728f-493a-a7a5-8c54fb293e31",
   "metadata": {},
   "source": [
    "The NN did do better with the test set in terms of accuracy but loss quite a bit worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881180a-bebe-43e2-8da9-f093b346bfa5",
   "metadata": {},
   "source": [
    "## ANN with Regularisation\n",
    "\n",
    "Ok, let's try smoothing some of that out with adding l2 regularisation to the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9a288-b92c-43cc-ae2a-467d66f297c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining model with L2 regularisation\n",
    "model_tf = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(100,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(100,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(100,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_tf.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "history = model_tf.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f32b7a-714b-4836-94b9-14acaa85a62e",
   "metadata": {},
   "source": [
    "val_loss: 0.4876 - val_accuracy: 0.8462 , so better than the last NN we build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4806d99-64d0-45ef-b4f5-74ae6c97d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8377f-a83d-46a9-a902-7167bedc01c3",
   "metadata": {},
   "source": [
    "That graph looks a lot better with some regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10322ce7-f9dd-483e-9733-b1b6f73702dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf.evaluate(X_test, y_test)\n",
    "annValidationRegularisationEval = model_tf.evaluate(X_test, y_test)\n",
    "print(\"ANN with Regularisation & Validation Loss: \", round(annValidationRegularisationEval[0],4), \"Accuracy: \", round(annValidationRegularisationEval[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Loss: \", round(logRegEval[0],4), \"Accuracy: \", round(logRegEval[1],4))\n",
    "print(\"ANN Loss: \", round(annEval[0],4), \"Accuracy: \", round(annEval[1],4))\n",
    "print(\"Logistic Regression with Validation Loss: \", round(logRegValidationEval[0],4), \"Accuracy: \", round(logRegValidationEval[1],4))\n",
    "print(\"ANN with Validation Loss: \", round(annValidationEval[0],4), \"Accuracy: \", round(annValidationEval[1],4))\n",
    "print(\"ANN with Regularisation & Validation Loss: \", round(annValidationRegularisationEval[0],4), \"Accuracy: \", round(annValidationRegularisationEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28544ca9",
   "metadata": {},
   "source": [
    "Interesting to see that ANN doesn't do the best in terms of loss but does well in accuracy performace - shows that optimising for loss and accuracy are not the same thing. Also shows that a simpler model can sometimes be better choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
