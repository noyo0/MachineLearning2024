{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf0aaf0-f556-4bf2-9a56-5e84e4096d55",
   "metadata": {},
   "source": [
    "# Binary Classification using Tensorflow\n",
    "\n",
    "We are going to do similar to the Regression Tensorflow workbook, but this time we are going to work at a classification problem\n",
    "\n",
    "Back to the titanic.csv file we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530745d-eefb-4de6-9b27-2c5ce3dc331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655b0f5-3c4d-4e03-8e14-4576b675ed28",
   "metadata": {},
   "source": [
    "### Read and clean data\n",
    "\n",
    "We'll need to read in the data and do the following\n",
    "- Remove any rows without Embarked filled in\n",
    "- Fill in any missing ages with a median value\n",
    "- Pick the columns we are going to build our model with\n",
    "- Encode some columns - convert from text to numbers\n",
    "  - Sex using label encoder\n",
    "  - Embarked using Onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea013f-077d-4450-be3c-b293dc82051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"titanic.csv\")\n",
    "df = df[df[\"Embarked\"].notnull()]   #Remove any rows without Embarked filled in\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())    #Fill in any missing ages with a median value\n",
    "\n",
    "#Pick the columns we are going to build our model with\n",
    "y = df[\"Survived\"]\n",
    "X = df[[\"Pclass\",\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e7a68",
   "metadata": {},
   "source": [
    "Convert sex to numeric values using label encoder from preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a4138-872f-467e-b527-697ec72162b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le_sex = preprocessing.LabelEncoder()\n",
    "le_sex.fit(X['Sex'])\n",
    "sex = le_sex.transform(X['Sex'])\n",
    "X = X.drop(['Sex'], axis = 1)\n",
    "X['Sex'] = sex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44252c-98fc-4357-b66e-5aafb4e05316",
   "metadata": {},
   "source": [
    "Convert Embarked to numeric values using OneHotEncoder from preprocessing library. There's probably a better way of doing this but this'll work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453c351-8119-459e-b2c1-8f7d441f1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_embark = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "le_embark.fit(X[\"Embarked\"].values.reshape(-1,1))\n",
    "embarked = le_embark.transform(X[\"Embarked\"].values.reshape(-1,1))\n",
    "X = X.drop([\"Embarked\"], axis = 1)\n",
    "X[\"EmbarkC\"] = embarked[:,0]\n",
    "X[\"EmbarkQ\"] = embarked[:,1]\n",
    "X[\"EmbarkS\"] = embarked[:,2]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee179b-a06e-4e03-ba2d-1b407471f4a3",
   "metadata": {},
   "source": [
    "### Machine Learning \n",
    "Then we can start doing our Machine Learning process\n",
    "- Train_test_split\n",
    "- Normalise the data\n",
    "- Write the base models\n",
    "- Compile\n",
    "- Fit\n",
    "- Evaluate\n",
    "Usual Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500f346-5a5f-47c6-932b-588b0753679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1138, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1ad22-5206-4101-982f-00809c56c08f",
   "metadata": {},
   "source": [
    "Normalisation like was done on the Regression workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cbf1f-3018-4129-8593-10718dfc5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(X_train.values)\n",
    "print('Normalized:', normalizer(X_train.values).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27792c3-0c0d-4585-b344-47343c857398",
   "metadata": {},
   "source": [
    "## Model Number 1 - Logistic Regression\n",
    "\n",
    "We are just going to do a regular Logistic Regression model to start with\n",
    "\n",
    "This just requires one output layer with one unit in it so it is the same as\n",
    "$$\\log\\left(\\frac{p}{1-p}\\right) = w^Tx + b $$\n",
    "\n",
    "This should be similar to using sklearn LogisticRegression()\n",
    "\n",
    "We put the normalizer in as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12cae7-fc05-45ad-b85e-a8d2b2c76503",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd6312-cbb5-4a77-89bf-c065a4581495",
   "metadata": {},
   "source": [
    "Trainable params is 10, we have 9 features (so 9 weights) and the bias b to be learnt as well\n",
    "\n",
    "Our loss is going to be https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy as it is a binary problem\n",
    "\n",
    "Read the manual even though the default is from_logits=False, they say Recommended Usage: (set from_logits=True (meaning output data is unscaled)). This depends on our output layer, if you look above I did not put any activation on the output layer, so the outputs will be logits i.e. from -infinity to +infinity\n",
    "\n",
    "If I put above tf.keras.layers.Dense(1, activation='sigmoid') then from_logits=False is needed, but it is not recommended according to the manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285c82f-461c-495b-8c51-87e4a50186e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be15f40-4b2d-4c9b-9115-28dca5d9b784",
   "metadata": {},
   "source": [
    "Loss keeps decreasing, haven't use a validation set though. Could've done a different number of epochs\n",
    "\n",
    "Be careful if you run .fit again. It continues on from where you left off, not starting from epoch 1 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90d59d-3fe6-43b4-b03d-44d2463ea150",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegEval = model.evaluate(X_test, y_test)\n",
    "print(\"Logistic Regression Loss: \", round(logRegEval[0],4), \"Accuracy: \", round(logRegEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ae2d8-721e-4f4b-942a-5c7e3f2015bd",
   "metadata": {},
   "source": [
    "## Model Number 2 - Neural Network\n",
    "\n",
    "Now we are going to build a Neural Network\n",
    "\n",
    "4 layers, 100 units in each hidden layer with relu as the activation function (you can try different structures if you want but for now I'm just demonstrating how Tensorflow works as well as some other things we can try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee31b6d-4495-45ee-836a-0fa7f3b34d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22117e31-a301-479d-96ed-33674ebea44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc128a47-61c8-4adb-825a-725999cc03ce",
   "metadata": {},
   "source": [
    "21301 trainable params, a lot more than the previous 10! Let's look at each layer\n",
    "\n",
    "Input has 9 features Hidden Layer 1 has 100 units. Therefore the weight matrix is going to have 100 rows and 9 columns to match\n",
    "$$ Wx + b $$\n",
    "\n",
    "That gives us 900 paramaters for the weights. Then we have 100 biases - giving the total of 1000 parameters to go from input layer to hidden layer 1\n",
    "\n",
    "Now from hidden layer 1 to hidden layer 2. 100 features, 100 units, gives us a 100x100 matrix so 10000 parameters, add in the 100 biases and we get the 10100\n",
    "\n",
    "Let's compile and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f2216-a1aa-43a6-979f-cebaa4154c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "model_tf.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8b694-eb05-45ae-a9c5-231691e6291b",
   "metadata": {},
   "source": [
    "Again Loss keeps decreasing, haven't use a validation set though. Could've done a different number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211012af-7694-430b-9f9f-7c08ded1eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Loss: \", round(logRegEval[0],4), \"Accuracy: \", round(logRegEval[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7e463-1636-434b-a476-233ba0f5c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annEval = model_tf.evaluate(X_test, y_test)\n",
    "print(\"ANN Loss: \", round(annEval[0],4), \"Accuracy: \", round(annEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6a46a-cbf0-42ce-98b1-5505f6f16f37",
   "metadata": {},
   "source": [
    "These are potentially overfit, we should have done some validation to check over things\n",
    "\n",
    "While the accuracy for the test data is better, the loss is actually worse!\n",
    "\n",
    "## Using Validation\n",
    "### Logistic Regression with Validation\n",
    "\n",
    "Let's add in validation. kFold Cross Validation would be nice but that's more difficult to do with tensorflow and requires writing our own functions so let's just take the last 20% as a validation set. Since we used train_test_split already, the data is already shuffled so it should be ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aaeb35-5091-445b-99a7-100826d32a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852303c9-a657-4fa4-a1e4-fbb95a3898cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0b326",
   "metadata": {},
   "source": [
    "Same function that was in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca7e16-2c5b-4579-bda3-af5bf4fe6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, which='loss'):\n",
    "    plt.plot(history.history[which], label='train')\n",
    "    try:\n",
    "        plt.plot(history.history['val_'+which], label='validation')\n",
    "    except:\n",
    "        None\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(which)\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523ef11-7739-43ab-bf87-d1ec0bf52a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033eddd8-fc0a-4df3-9428-3e9a237ff951",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegValidationEval = model.evaluate(X_test, y_test)\n",
    "print(\"Logistic Regression with Validation Loss: \", round(logRegValidationEval[0],4), \"Accuracy: \", round(logRegValidationEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abc82a-fee7-4f07-a857-0443af6f8af2",
   "metadata": {},
   "source": [
    "Seems about the same performance on the test set. Your numbers will vary due to SGD\n",
    "\n",
    "Now ANN\n",
    "\n",
    "### ANN with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712facf0-5db9-4e0a-ae9c-8ed7432e8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_tf.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "history = model_tf.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb678ab-f08b-4659-b98c-0f17ce9a832a",
   "metadata": {},
   "source": [
    "Let's plot the loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280fb6a-0ca4-4caf-b68c-b38e0c004e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a7fda-c9ca-4352-a32e-2a0ad789e637",
   "metadata": {},
   "source": [
    "Validation loss is a bit all over the place. This suggests overfitting to me\n",
    "\n",
    "Neural Networks are not always better....even though the training loss and accuracy was better with the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020f88a-83e8-4250-a54d-16e32e1f4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f133db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression with Validation Loss: \", round(logRegValidationEval[0],4), \"Accuracy: \", round(logRegValidationEval[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb4e4a-51bd-42d1-9c41-1ea1b05b273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annValidationEval = model_tf.evaluate(X_test, y_test)\n",
    "print(\"ANN with Validation Loss: \", round(annValidationEval[0],4), \"Accuracy: \", round(annValidationEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9840a0-728f-493a-a7a5-8c54fb293e31",
   "metadata": {},
   "source": [
    "The NN did do better with the test set in terms of accuracy but loss quite a bit worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881180a-bebe-43e2-8da9-f093b346bfa5",
   "metadata": {},
   "source": [
    "## ANN with Regularisation\n",
    "\n",
    "Ok, let's try smoothing some of that out with adding l2 regularisation to the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9a288-b92c-43cc-ae2a-467d66f297c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining model with L2 regularisation\n",
    "model_tf = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(100,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(100,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(100,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_tf.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics='accuracy',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "history = model_tf.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f32b7a-714b-4836-94b9-14acaa85a62e",
   "metadata": {},
   "source": [
    "val_loss: 0.4876 - val_accuracy: 0.8462 , so better than the last NN we build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4806d99-64d0-45ef-b4f5-74ae6c97d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8377f-a83d-46a9-a902-7167bedc01c3",
   "metadata": {},
   "source": [
    "That graph looks a lot better with some regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10322ce7-f9dd-483e-9733-b1b6f73702dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf.evaluate(X_test, y_test)\n",
    "annValidationRegularisationEval = model_tf.evaluate(X_test, y_test)\n",
    "print(\"ANN with Regularisation & Validation Loss: \", round(annValidationRegularisationEval[0],4), \"Accuracy: \", round(annValidationRegularisationEval[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Loss: \", round(logRegEval[0],4), \"Accuracy: \", round(logRegEval[1],4))\n",
    "print(\"ANN Loss: \", round(annEval[0],4), \"Accuracy: \", round(annEval[1],4))\n",
    "print(\"Logistic Regression with Validation Loss: \", round(logRegValidationEval[0],4), \"Accuracy: \", round(logRegValidationEval[1],4))\n",
    "print(\"ANN with Validation Loss: \", round(annValidationEval[0],4), \"Accuracy: \", round(annValidationEval[1],4))\n",
    "print(\"ANN with Regularisation & Validation Loss: \", round(annValidationRegularisationEval[0],4), \"Accuracy: \", round(annValidationRegularisationEval[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28544ca9",
   "metadata": {},
   "source": [
    "Interesting to see that ANN doesn't do the best in terms of loss but does well in accuracy performace - shows that optimising for loss and accuracy are not the same thing. Also shows that a simpler model can sometimes be better choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
